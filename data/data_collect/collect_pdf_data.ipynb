{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metadata의 entry_id를 기준으로 논문의 pdf 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "import arxiv\n",
    "import itertools\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load meatadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../data_files/paper_metadata/arxiv_ascending_20161001_20230920.pkl\", 'rb') as f:\n",
    "    paper_meta = pickle.load(f)\n",
    "    \n",
    "with open (\"../data_files/paper_metadata/arxiv_descending_2024_20160927.pkl\", 'rb') as f:\n",
    "    after2023 = pickle.load(f)\n",
    "\n",
    "paper_meta.extend(after2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집된 metadata 수:  70000\n",
      "기간:  2016-10-01 ~ 2024-12-24\n"
     ]
    }
   ],
   "source": [
    "print(\"수집된 metadata 수: \", len(paper_meta))\n",
    "sorted_paper = sorted(paper_meta, key=lambda x: x['published'])\n",
    "\n",
    "pub_dates = [datetime.strftime(paper['published'], format=\"%Y-%m-%d\")  for paper in sorted_paper]\n",
    "print(\"기간: \", sorted(pub_dates)[0], '~', sorted(pub_dates)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id로 주소 불러오고 제목을 기준으로 저장하기 위한 dict \n",
    "\n",
    "title_id = {}\n",
    "\n",
    "for paper in paper_meta:\n",
    "    tmp_title = paper['title']\n",
    "    tmp_id = paper['entry_id'].split('/')[-1]\n",
    "    title_id[tmp_id] = tmp_title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목 필터링 함수\n",
    "def sanitize_title(title):\n",
    "    title_fixed = title.replace('\\n', ' ').strip()  # 줄바꿈 제거 및 양끝 공백 제거\n",
    "    title_fixed = re.sub(r'\\s+', ' ', title_fixed)  # 여러 공백을 하나로 축소\n",
    "    title_fixed = title_fixed.replace('/', '_')  # '/'를 '_'로 대체\n",
    "    return title_fixed\n",
    "\n",
    "# 논문 검색 및 다운로드 함수\n",
    "def fetch_and_download(key, title, dir_name, error_dict):\n",
    "    try:\n",
    "        tmp_paper = next(arxiv.Client().results(arxiv.Search(id_list=[key])))\n",
    "        title_fixed = sanitize_title(title)\n",
    "        \n",
    "        # PDF 다운로드\n",
    "        tmp_paper.download_pdf(dirpath=dir_name, filename=f\"{title_fixed}.pdf\")\n",
    "        time.sleep(2)  # 요청 간격 2초\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_dict[key] = title\n",
    "        print(f\"Error downloading paper with ID {key}: {e}\")\n",
    "        return False\n",
    "    \n",
    "# 디렉토리 생성 함수\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 4    \n",
    "batch_size = 5000  \n",
    "\n",
    "start_val = 0  # 시작 인덱스\n",
    "end_val = 70000    # 끝 인덱스\n",
    "\n",
    "for batch_start in range(start_val, end_val, batch_size):\n",
    "    batch_end = batch_start + batch_size\n",
    "    if batch_end > end_val:\n",
    "        batch_end = end_val  # 55,000을 초과하지 않도록 처리\n",
    "\n",
    "    batch_error_ids = {}\n",
    "\n",
    "    dir_name = f\"./paper_pdf_{batch_end}\"\n",
    "    ensure_dir(dir_name)\n",
    "\n",
    "    # 슬라이싱할 데이터\n",
    "    title_id_items = list(itertools.islice(title_id.items(), batch_start, batch_end))\n",
    "\n",
    "    print(f\"\\n=== Downloading batch {batch_start} ~ {batch_end} (총 {len(title_id_items)}개) ===\")\n",
    "\n",
    "    # 병렬 다운로드 실행\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda item: fetch_and_download(item[0], item[1], dir_name, batch_error_ids),\n",
    "                title_id_items\n",
    "            ),\n",
    "            total=len(title_id_items)\n",
    "        ))\n",
    "\n",
    "    # 에러 개수 계산\n",
    "    batch_error_count = len(batch_error_ids)\n",
    "    print(f\"Batch {batch_start}~{batch_end} completed with {batch_error_count} errors.\\n\")\n",
    "\n",
    "    # Pickle로 배치별 에러 저장\n",
    "    with open(f'error_id_during_save_pdf_{batch_start}_{batch_end}.pkl', 'wb') as f:\n",
    "        pickle.dump(batch_error_ids, f)\n",
    "\n",
    "print(\"All done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
