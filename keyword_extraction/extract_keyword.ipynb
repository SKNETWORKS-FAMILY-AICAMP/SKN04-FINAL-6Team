{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract, Introduction, Related works / Background 를 통해 keyword 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ray\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load paper parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70000개의 논문에 대해서 저장한 것\n",
    "saved_df = pd.read_parquet('../data/data_files/filtered_data/processed_papers.parquet')\n",
    "\n",
    "# related_work 혹은 background 둘 다 없는 경우, introduction 없는 경우 모두 제거 후 사용 \n",
    "nothing_df = saved_df[(saved_df['related_work'].str.startswith(\"CODE\")) & (saved_df['background'].str.startswith(\"CODE\"))]\n",
    "no_intro = saved_df[saved_df['introduction'].str.startswith(\"CODE\")]\n",
    "\n",
    "nothig_df_idx = list(nothing_df.index)\n",
    "no_intro_idx = list(no_intro.index)\n",
    "\n",
    "union_index = list(set(nothig_df_idx).union(set(no_intro_idx)))\n",
    "paper_df = saved_df.drop(index=union_index)\n",
    "paper_df = paper_df.sort_values(by=['published']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문 데이터\n",
    "paper_docs = []\n",
    "for idx in paper_df.index:\n",
    "    row_contents = []\n",
    "    tmp_rows = paper_df.iloc[idx]\n",
    "    \n",
    "    row_abstract = tmp_rows['abstract']\n",
    "    row_introduction = tmp_rows['introduction']\n",
    "    row_related_work = tmp_rows['related_work']\n",
    "    row_background = tmp_rows['background']\n",
    "    \n",
    "    row_contents.append(f\"[ABSTRACT]: {row_abstract}\")\n",
    "    \n",
    "    if not row_introduction.startswith(\"CODE\"):\n",
    "        row_contents.append(f\"[INTRODUCTION]: {row_introduction}\")\n",
    "    \n",
    "    if not row_related_work.startswith(\"CODE\"):\n",
    "        row_contents.append(f\"[RELATED_WORKS]: {row_related_work}\")\n",
    "        \n",
    "    if not row_background.startswith(\"CODE\"):\n",
    "        row_contents.append(f\"[BACKGROUND]: {row_background}\")\n",
    "    \n",
    "    row_contents_str = ''.join(row_contents)\n",
    "    paper_docs.append(row_contents_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 사용하여 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM에 원문 입력\n",
    "def extract_keyword_using_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "    문서의 구조는 다음과 같습니다:\n",
    "    - [ABSTRACT]: 논문의 초록으로 현재 연구의 주제와 핵심 아이디어를 요약합니다.\n",
    "    - [INTRODUCTION]: 연구의 배경과 목표를 소개하며, 현재 연구의 주요 내용과 동기를 설명합니다.\n",
    "    - [RELATED_WORKS]: 이전 연구를 요약하고, 현재 연구와의 차별성을 논의합니다.\n",
    "    - [BACKGROUND]: 연구의 기초 개념과 관련 정보를 제공합니다.\n",
    "\n",
    "    현재 문서의 내용을 기반으로 아래 작업을 수행하세요:\n",
    "    1. ABSTRACT와 INTRODUCTION를 참고하여 현재 연구를 가장 잘 대표하는 키워드를 최대 2개 추출하세요.\n",
    "    2. RELATED WORKS와 BACKGROUND를 참고하여 현재 연구 이전의 선행 연구 키워드를 추출하세요.\n",
    "    - 이전의 선행 연구 키워드는 1번에서의 대표 키워드와 중복될 수 없다. \n",
    "    - 모든 키워드는 특정 언어에 국한되지 않도록 일반화 과정을 추가한다. \n",
    "        예시: 'Arabic-Hebrew parallel corpus' 는 'parallel corpus' 와 같이 치환\n",
    "    - 단 데이터셋(Dataset)이 키워드로 들어가는 경우에는 제거한다.\n",
    "    3. 최종적으로 1번과 2번 중 연관도가 높은 키워드 5개 추출하여 dictionary 형태로 출력합니다,\n",
    "    모든 키워드는 영어로 되어있으며, 소문자로 답한다.\n",
    "    결과를 **JSON 형식**으로만 반환하세요. 텍스트로 된 설명이나 코드 블록은 포함하지 마세요. 예시는 다음과 같습니다:\n",
    "    {{\n",
    "        \"main_keywords\": [\"keyword1\", \"keyword2\"],\n",
    "        \"prev_keywords\": [\"keyword3\", \"keyword4\", \"keyword5\", \"keyword6\", \"keyword7\"]\n",
    "    }}\n",
    "    문서: {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"학술 논문에 대해서 잘 알고있으며, 논문 내 키워드 관계 분석 전문가입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p=1\n",
    "    )\n",
    "\n",
    "    return(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word embeddings', 'sense embeddings']\n",
      "['semantic networks', 'lexical resources', 'disambiguation', 'neural architectures', 'vector space']\n",
      "['entity identification', 'neural architecture']\n",
      "['named-entity recognition', 'sequence labeling', 'structured prediction', 'multitasking', 'bidirectional lstm']\n",
      "['neural execution', 'natural language querying']\n",
      "['symbolic execution', 'reinforcement learning', 'knowledge base', 'neural networks', 'program synthesis']\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "for idx in range(len(paper_docs))[222:225]:\n",
    "    tmp_text = paper_docs[idx]\n",
    "    tmp_json_result = extract_keyword_using_gpt(tmp_text)\n",
    "    tmp_result = json.loads(tmp_json_result)\n",
    "    paper_keywords = tmp_result.get(\"main_keywords\")\n",
    "    prev_keywords = tmp_result.get(\"prev_keywords\")\n",
    "    \n",
    "    paper_df.loc[idx, 'main_keywords'] = str(paper_keywords)\n",
    "    paper_df.loc[idx, 'prev_keywords'] = str(prev_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 데이터에 대해서 키워드 추출 (병렬처리를 위해 ray 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=5, ignore_reinit_error=True)  # 최대 4개의 CPU 사용\n",
    "\n",
    "@ray.remote(max_retries=3)\n",
    "def process_document(idx, text):\n",
    "    try:\n",
    "        tmp_json_result = extract_keyword_using_gpt(text)\n",
    "        tmp_result = json.loads(tmp_json_result)\n",
    "        paper_keywords = tmp_result.get(\"main_keywords\", [])\n",
    "        prev_keywords = tmp_result.get(\"prev_keywords\", [])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx}: {e}\")  # 디버깅 정보 출력\n",
    "        paper_keywords, prev_keywords = [], []\n",
    "    return idx, paper_keywords, prev_keywords\n",
    "\n",
    "# 데이터를 준비\n",
    "data = [(idx, paper_docs[idx]) for idx in range(len(paper_docs))]\n",
    "\n",
    "# CPU 수에 맞는 배치 크기 설정\n",
    "# 너무 크게 잡으면 결과값에 None 나옴\n",
    "num_cpus = 5\n",
    "batch_size = max(1, 300 // num_cpus)  # 배치 크기를 CPU 수에 따라 조정\n",
    "\n",
    "results = []\n",
    "saved_batches = 0\n",
    "\n",
    "for start in range(0, len(data), batch_size):\n",
    "    end = min(start + batch_size, len(data))\n",
    "    batch = data[start:end]  # batch 데이터를 슬라이스\n",
    "    futures = [process_document.remote(idx, text) for idx, text in batch]\n",
    "    batch_results = ray.get(futures)\n",
    "    \n",
    "    # 배치 결과 저장\n",
    "    batch_df = pd.DataFrame(batch_results, columns=[\"idx\", \"main_keywords\", \"prev_keywords\"])\n",
    "    batch_filename = f\"../data/data_files/keyword_parquet_backup/results_{start}_{end}.parquet\"  # start와 end를 파일명에 포함\n",
    "    batch_df.to_parquet(batch_filename, index=False)  # 파일로 저장\n",
    "\n",
    "    print(f\"Batch {start}-{end} saved to {batch_filename}.\")\n",
    "    results.extend(batch_results)  # 전체 결과에 추가\n",
    "    \n",
    "     # 결과가 1000개 단위마다\n",
    "    if len(results) % 1000 == 0:\n",
    "        batch_df = pd.DataFrame(results, columns=[\"idx\", \"main_keywords\", \"prev_keywords\"])\n",
    "        batch_filename = f\"../data/data_files/keyword_parquet_per_batch/results_cumulative_batch_{saved_batches}.parquet\"\n",
    "        batch_df.to_parquet(batch_filename, index=False)\n",
    "        print(f\"Cumulative results saved to {batch_filename}.\")\n",
    "        \n",
    "        results = []  # 저장 후 누적 리스트 초기화\n",
    "        saved_batches += 1  # 저장된 배치 수 증가\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "# 디버깅 결과 출력\n",
    "for idx, paper_keywords, prev_keywords in results:\n",
    "    if not paper_keywords and not prev_keywords:\n",
    "        print(f\"No keywords found for document {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../data/data_files/keyword_parquet_per_batch/'\n",
    "keyword_df = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\") and \"results_\" in file_name:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        tmp_df = pd.read_parquet(file_path)\n",
    "        keyword_df = pd.concat([keyword_df, tmp_df], ignore_index=True)\n",
    "\n",
    "keyword_df = keyword_df.sort_values(by=['idx']).reset_index(drop=True)\n",
    "keyword_df.to_parquet('../data/data_files/keyword_data/keyword_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.merge(paper_df, keyword_df, left_index=True, right_index=True)\n",
    "total_df = total_df.drop(columns=['idx'])\n",
    "total_df.to_parquet('../data/data_files/keyword_data/total_df_with_keyword.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
